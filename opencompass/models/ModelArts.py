import json
import time
import requests
from typing import List, Union, Dict, Optional
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import logging

# å‡è®¾ä½ å·²æœ‰ BaseAPIModel å’Œç›¸å…³ç±»åž‹å®šä¹‰
from .base_api import BaseAPIModel  # è¯·æ ¹æ®ä½ çš„é¡¹ç›®ç»“æž„è°ƒæ•´å¯¼å…¥è·¯å¾„
from opencompass.utils import PromptList
from opencompass.utils.logging import get_logger

PromptType = Union[PromptList, str]

# åˆ›å»ºä¸€ä¸ªç¦ç”¨çš„æ—¥å¿—å™¨ï¼ˆç”¨äºŽé™é»˜æ¨¡å¼ï¼‰
DISABLED_LOGGER = logging.getLogger('disabled')
DISABLED_LOGGER.disabled = True

logger = get_logger()


class HuaweiModel(BaseAPIModel):
    """Model class for Huawei Model API interaction"""

    is_api: bool = True

    def __init__(self,
                 path: str,
                 model: str = "Qibo",
                 api_base: str = "http://localhost:5000/tohuawei",
                 query_per_second: int = 1,
                 rpm_verbose: bool = False,
                 retry: int = 3,
                 max_seq_len: int = 2048,
                 meta_template: Optional[Dict] = None,
                 generation_kwargs: Dict = dict(),
                 mode: str = 'none',
                 tokenizer_path: Optional[str] = None,
                 verbose: bool = True):
        super().__init__(
            path=path,
            max_seq_len=max_seq_len,
            meta_template=meta_template,
            query_per_second=query_per_second,
            rpm_verbose=rpm_verbose,
            retry=retry,
            verbose=verbose,
        )

        self.model_name = model
        self.api_base = api_base
        self.generation_kwargs = generation_kwargs
        self.headers = {"Content-Type": "application/json"}
        self.logger = get_logger(__name__) if verbose else DISABLED_LOGGER

        self._log_init_info()

    def _log_init_info(self):
        """æ‰“å°æ¨¡åž‹åˆå§‹åŒ–ä¿¡æ¯"""
        self.logger.info(" HuaweiModel åˆå§‹åŒ–æˆåŠŸ ".center(60, "-"))
        self.logger.info(f"ðŸ”Œ API åœ°å€: {self.api_base}")
        self.logger.info(f"ðŸ¤– æ¨¡åž‹åç§°: {self.model_name}")
        self.logger.info(f"ðŸ” æœ€å¤§é‡è¯•æ¬¡æ•°: {self.retry}")
        self.logger.info(f"â±ï¸ æ¯ç§’è¯·æ±‚æ•° (QPS): {self.query_per_second}")
        self.logger.info("-" * 60)

    def _build_payload(self, prompt: str, max_out_len: int, temperature: float) -> dict:
        """
        æž„å»ºè¯·æ±‚ä½“ï¼Œé»˜è®¤åªä¼ å…¥ query å­—æ®µã€‚
        ä½ å¯ä»¥æ ¹æ®å®žé™…æŽ¥å£æ–‡æ¡£æ‰©å±•å‚æ•°ã€‚
        """
        return {
            "query": prompt
        }

    def _send_request(self, payload: dict) -> Optional[dict]:
        """å‘é€ HTTP è¯·æ±‚å¹¶å…¼å®¹æ€§ä¿®å¤å•å¼•å·é—®é¢˜"""
        for attempt in range(1, self.retry + 1):
            try:
                time.sleep(1 / self.query_per_second)
                response = requests.post(
                    self.api_base,
                    headers=self.headers,
                    data=json.dumps(payload),
                    timeout=120
                )
                print(f"[DEBUG] Status code: {response.status_code}")
                print(f"[DEBUG] Raw response text: {response.text}")

                # å°è¯•ä¿®å¤ä¸åˆæ³• JSONï¼ˆå¦‚å•å¼•å·ï¼‰
                content = response.text.replace("'", '"')
                return json.loads(content)

            except Exception as e:
                print(f"[ERROR] Request failed (attempt {attempt}): {e}")
                time.sleep(2 ** attempt)
        return None

    def _extract_text_from_response(self, response: dict) -> str:
        """
        å®‰å…¨æå– response ä¸­çš„ç”Ÿæˆæ–‡æœ¬å†…å®¹

        æ”¯æŒæ ¼å¼ï¼š
          - response['choices'][0]['text']['conntent']
          - response['choices'][0]['text']
          - response['text']
          - response['output']
        """
        if not isinstance(response, dict):
            return ""

        # æ–¹æ³•ä¸€ï¼šå°è¯•ä»Ž choices æå–ï¼ˆæ³¨æ„å¤„ç† text æ˜¯ dict çš„æƒ…å†µï¼‰
        try:
            content = response['choices'][0]['text'] # å¤šå±‚å®‰å…¨èŽ·å–
            if isinstance(content, str):
                return content.strip()
        except Exception as e:
            pass

        # å¦‚æžœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›žç©ºå­—ç¬¦ä¸²
        self.logger.warning("âš ï¸ æ— æ³•ä»Ž response ä¸­æå–ç”Ÿæˆæ–‡æœ¬")
        return "å¤±è´¥"

    def _process_response(self, response: dict) -> str:
        """è§£æžå“åº”å†…å®¹å¹¶æå–ç”Ÿæˆæ–‡æœ¬"""
        generated_text = self._extract_text_from_response(response)
        self.logger.info(f"ðŸ“¥ æŽ¥æ”¶åˆ°å“åº”å†…å®¹: {generated_text}")
        print("ðŸ” åŽŸå§‹ response å†…å®¹:", response)
        return generated_text

    def _generate_single(self, prompt: str, max_out_len: int, temperature: float) -> str:
        """å•æ¡ç”Ÿæˆé€»è¾‘"""
        payload = self._build_payload(prompt, max_out_len, temperature)
        raw_response = self._send_request(payload)
        if raw_response:
            return self._process_response(raw_response)
        return "è°ƒç”¨å¤±è´¥"

    def generate(
        self,
        inputs: List[PromptType],
        max_out_len: int = 8192,
        temperature: float = 0.7,
        **kwargs,
    ) -> List[str]:
        """æ‰¹é‡ç”ŸæˆæŽ¥å£"""
        temperature = self.generation_kwargs.get('temperature', temperature)
        self.logger.info(f"ðŸ”¥ å¼€å§‹æ‰¹é‡ç”Ÿæˆï¼Œå…± {len(inputs)} æ¡è¾“å…¥ï¼Œæ¸©åº¦: {temperature}")

        with ThreadPoolExecutor() as executor:
            results = list(tqdm(
                executor.map(
                    lambda x: self._generate_single(x, max_out_len, temperature),
                    inputs
                ),
                total=len(inputs),
                desc="HuaweiModel Inferencing"
            ))

        return results

    def get_token_len(self, prompt: str) -> int:
        """ä¼°ç®— token é•¿åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return len(prompt) // 4  # ç²—ç•¥ä¼°ç®—